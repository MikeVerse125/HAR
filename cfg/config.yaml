eperiment: attention

data: 
  csv_file: "A1_2024_data/train_data_2024.csv"
  img_dir: "A1_2024_data/Images"
  batch_size: 16
  num_workers: 4 
  num_classes: 40 
  augmentation: 
    resize: [224, 224]
    normaliszation:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    horizontal_flip: true
    rotation: 15

training:
  epochs: 20
  learning_rate: 1e-4    
  save_model: true
  early_stopping: true
  patience: 5
  classification_weight: 1.0
  binary_weight: 0.5
  status: true
  track_history: true
  use_scheduler: true
  scheduler:
    type: "ReduceLROnPlateau"  # Options: ReduceLROnPlateau, CosineAnnealingLR, StepLR
    mode: "min"                # For ReduceLROnPlateau
    factor: 0.5                # Multiply LR by this when plateauing
    patience: 3                # Epochs to wait before reducing LR
    verbose: true              # Print LR changes

model:
  type: attention # [baseline, attention]
  params: 
    reduction_ratio: 16
    kernel_size: 7
    finetune: true # If true all layer on the pretrain model unfreeze 
    dropout_rate: 0.5
    in_channels: [128, 64, 8]
  pretrained_model_path: "results/ModelsSaved/best_attention.pth"  # Used when finetune=true
    

evaluation: 
  metrics: [loss, f1_score]           # [loss, f1_score, precision, recall]
  heads: ["classification", "binary"]  # ["classification", "binary"] 
  save_results: true
  status: true

plotter: 
  status: true

prediction:
  num_of_predimg: 9 # Maximum is 9
  model_path: "results/ModelsSaved/best_fine_tune_attention.pth"
